{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7a0466",
   "metadata": {},
   "source": [
    "## Checkpoints Analysis on Keras using Microscopy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a6b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.utils import  img_to_array,  load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7f859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 150\n",
    "###2 conv and pool layers. with some normalization and drops in between.\n",
    "INPUT_SHAPE = (SIZE, SIZE, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776c5ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 148, 148, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 72, 72, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 36, 36, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 34, 34, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 17, 17, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 18496)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1183808   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=INPUT_SHAPE))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5055cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e81d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "# Training augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=45,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327e4ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5600 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create Generator \n",
    "# This will read pictures found in subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/cell_images/train',  # directory\n",
    "        target_size=(150, 150),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc8e66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is a similar generator, for validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'data/cell_images/test',  # directory\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6153f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Callbacks: ModelCheckpoints\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d987500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ModelCheckpoint callback saves a model at some interval.\n",
    "filepath=\"saved_models/jupnote_model.{epoch:02d}-{val_accuracy:.2f}.h5\" #File name includes epoch and validation accuracy.\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"val_accuracy\", verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ebf8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our checkpoint for callback list, may choose to add early stopping soon..\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31b21588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8490\n",
      "Epoch 1: val_accuracy improved from -inf to 0.60000, saving model to saved_models\\jupnote_model.01-0.60.h5\n",
      "200/200 [==============================] - 35s 169ms/step - loss: 0.3696 - accuracy: 0.8490 - val_loss: 5.2225 - val_accuracy: 0.6000\n",
      "Epoch 2/6\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9865\n",
      "Epoch 2: val_accuracy improved from 0.60000 to 0.64000, saving model to saved_models\\jupnote_model.02-0.64.h5\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 0.2653 - accuracy: 0.9865 - val_loss: 3.6381 - val_accuracy: 0.6400\n",
      "Epoch 3/6\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9920\n",
      "Epoch 3: val_accuracy did not improve from 0.64000\n",
      "200/200 [==============================] - 33s 165ms/step - loss: 0.2114 - accuracy: 0.9920 - val_loss: 3.8347 - val_accuracy: 0.6000\n",
      "Epoch 4/6\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.9940\n",
      "Epoch 4: val_accuracy did not improve from 0.64000\n",
      "200/200 [==============================] - 33s 166ms/step - loss: 0.1313 - accuracy: 0.9940 - val_loss: 7.0758 - val_accuracy: 0.6000\n",
      "Epoch 5/6\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9875\n",
      "Epoch 5: val_accuracy did not improve from 0.64000\n",
      "200/200 [==============================] - 33s 167ms/step - loss: 0.2349 - accuracy: 0.9875 - val_loss: 5.1619 - val_accuracy: 0.6400\n",
      "Epoch 6/6\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.0843 - accuracy: 0.9955\n",
      "Epoch 6: val_accuracy did not improve from 0.64000\n",
      "200/200 [==============================] - 35s 175ms/step - loss: 0.0843 - accuracy: 0.9955 - val_loss: 9.3358 - val_accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "#We can now use these generators to train our model.\n",
    "history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=6,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks_list)\n",
    "model.save('malaria_augmented_model.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0633d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 67ms/step - loss: 2.0452e-06 - accuracy: 1.0000\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 9.3358 - accuracy: 0.6000\n",
      "Train: 1.000, Test: 0.600\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc = model.evaluate(train_generator, steps=15)\n",
    "validation_loss, test_acc = model.evaluate(validation_generator, steps=10)\n",
    "\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a0df78",
   "metadata": {},
   "source": [
    "Let us try reloading the exported model and check for accuracy againts the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6b973c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model(\"saved_models/jupnote_model.05-0.72.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2755064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 30ms/step - loss: 3.9511 - accuracy: 0.7200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.9510796070098877, 0.7200000286102295]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.evaluate(validation_generator, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32c87c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 59ms/step - loss: 3.9596e-09 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.959585104240659e-09, 1.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.evaluate(train_generator, steps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99759da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.save('saved_models/loaded_model_malaria_augmented_model.h5') \n",
    "loaded_model_2 = load_model(\"saved_models/loaded_model_malaria_augmented_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b50db20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5700 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "all_generator = validation_datagen.flow_from_directory(\n",
    "        'data/cell_images/all',  # directory\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d47718b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 31ms/step - loss: 0.0788 - accuracy: 0.9933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0787973701953888, 0.9933333396911621]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_2.evaluate(all_generator, steps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2cc64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
